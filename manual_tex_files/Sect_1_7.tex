%
\documentclass[11pt]{report}
\usepackage{geometry} 
\geometry{letterpaper}

%---------------------------------------------
\setlength{\textheight}{630pt}
\setlength{\textwidth}{450pt}
\setlength{\oddsidemargin}{14pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}


%----------------------------------------
\usepackage{amsmath}
\usepackage{layout}
\usepackage{color}
\usepackage{hyphenat}

%----------------------------------------------
\usepackage{fancyhdr} \pagestyle{fancy}
\setlength\headheight{15pt}
\lhead{\small{User's Guide - \textit{WARP3D}}}
\rhead{\small{\textit{Linear Equation Solvers}}}
\fancyfoot[L] {\small{\textit{Chapter {\thechapter}}\ \   (Updated: 5-9-2013)}}
\fancyfoot[C] {\small{\thesection-\thepage}}
\fancyfoot[R] {\small{\textit{Introduction}}}

%---------------------------------------------------
\usepackage{graphicx}
\usepackage[labelformat=empty]{caption}
\numberwithin{equation}{section}

%---------------------------------------------
%     --- make section headers in helvetica ---
%
\frenchspacing
\usepackage{sectsty} 
\usepackage{xspace}
\allsectionsfont{\sffamily} 
\sectionfont{\large}
\usepackage[small,compact]{titlesec} % reduce white space around sections
%
%----------------------------------------------

%---------  local commands ---------------------
\newcommand{\tb} {\textbf}
\newcommand{\df} {\dotfill}
\newcommand{\nin} {\noindent}
\newcommand{\bmf } {\boldsymbol }  %bold math symbol
\newcommand{\bsf } [1]{\textrm{\textit{#1}}\xspace}
\newcommand{\ul} {\underline}
\newcommand{\hv} {\mathsf}   %helvetica text inside an equation
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ti}{\emph}
\newcommand{\bardelta}{\bar \delta}
\newcommand{\barDelta}{\bar \Delta}

\newenvironment{offsetpar}[1]%
{\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
}
{\end{list}}

%
%
%        optional definition for bullet lists which
%        reduces white space.
%
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }
%

%-------------------------------------
\newcounter{sectrefs}
\setcounter{sectrefs}{0}
\setcounter{chapter}{1}
\setcounter{section}{6}

%--------------------------------------
%--------------------------------------
%---------------------------------------

\begin{document}

\section{Linear Equation Solvers}
%\layout
\subsection{Overview}
\noindent
Solution of the linear set of equations described by Eq. (1.36) within each global Newton
iteration is accomplished using one of several available techniques as described here.
All of these solvers execute in parallel using a variety of techniques based on the
available computer hardware. Chapter 7 provides more details on starting WARP3D
for execution using parallel methods.

\subsection{Pardiso Sparse Direct Solver: Threads only}
\nin
The Pardiso sparse direct solver for symmetric systems of equations [\ref{R:PA1},
\ref{R:PA2}]
is implemented in the Intel Math Kernel Library (MKL) [\ref{R:Intel-MKL}]. 
This solver runs parallel using threads and shared memory. An out-of-memory
option is available for machines with a reduced amount of real memory.

\subsection{Pardiso Sparse Iterative Solver: Threads only}
\nin
The iterative form of the Pardiso sparse iterative solver  for symmetric systems of equations
is also implemented in the Intel Math Kernel Library (MKL). 
This solver runs parallel using threads and shared memory. The solver employs
a Krylov subspace method with conjugate gradient iterations.
The fully factored equations serve as the
preconditioner. The sparse direct solver described above performs
the factorization. At present, this solver runs with a fixed preconditioner for load
steps and Newton iterations until the convergence rate of the CG
iterations degrades to an unacceptable level. Then the full set of equations
is re-factored to update the preconditioner. This solver is
often twice (or more) as fast for larger models as the direct solver since CG iterations are much
less expensive that factorization. The solver always returns a solution via 
factorization if the CG iterations fail to converge.

\subsection{Hypre Iterative Solver: MPI only}
\nin
This solver is available only with the \ti{hybrid} (MPI + threads) version of WARP3D.
The \ti{hypre} iterative solver is developed and made available from Lawrence Livermore
National Laboratory [\ref{R:Hypre1}]. This solver is
suitable for models with millions of nodal displacements. 
The finite element model must be partitioned into domains where the number of domains
equals the number of MPI ranks (processes) specified for execution. Hypre runs
on the same ranks as WARP3D. On each MPI rank, threads may be employed for an
additional level of parallel execution both in WARP3D and hypre.




\subsection{Recommendations}
\nin
We expect most users will run WARP3D on shared-memory computers
having multiple processors each with multiple cores capable of running 
multiple threads. These include
laptops, desktop and deskside machines running Windows, OS X or Linux.
No domain decomposition of the model is employed and the assignment of 
elements into blocks can be performed directly by WARP3D.
The Pardiso direct solver then becomes the first choice among equation solvers. 
Use of the Pardiso sparse iterative solver requires only an option word in the solver
command. The iterative solver is worth exploring for possible gains in 
efficiency (especially
for linear and nonlinear dynamic analyses).

For the very largest models having perhaps millions of equations, the hypre 
solver is strongly recommended. Models of this size require computers
with significantly more capability (distributed clusters)
that current desktop/deskside or even
local workgroup computers. Hypre is available only in the hybrid
version of WARP3D (MPI+threads). Although designed for the solution of very large models on
clusters, hypre will run on all levels of hardware, including multicore laptops.
%*****************************************************
\subsection {References}
%*****************************************************
\small

[\refstepcounter{sectrefs}\label {R:PA1}\thesectrefs]~O. Schenk and K. Gartner. 
Solving Unsymmetric 
Sparse Systems of Linear Equations with PARDISO. \ti{ Journal of Future Generation Computer Systems},
 20(3):475--487, 2004.
 
\medskip\noindent
[\refstepcounter{sectrefs}\label {R:PA2}\thesectrefs]~O. Schenk and K. Gartner.
On fast factorization pivoting methods for symmetric indefinite systems, 
\ti{Elec. Trans. Numer. Anal.},
23:158--179, 2006.

\medskip
\noindent[\refstepcounter{sectrefs}\label {R:Intel-MKL}\thesectrefs]~Intel. 
http://software.intel.com/en-us/articles/intel-mkl/

\medskip
\noindent[\refstepcounter{sectrefs}\label {R:Hypre1}\thesectrefs]~Lawrence
Livermore national Laboratory. \\https://computation.llnl.gov/casc/sc2001\_fliers/hypre/
hypre01.html.

\medskip
\noindent[\refstepcounter{sectrefs}\label {R:EBE-Concus}\thesectrefs]~Concus P., 
Golub G.H., and OÕLeary D.P.
A Generalized Conjugate Gradient Method for the Numerical Solution 
of Elliptic Partial Differential Equation.
\ti{Sparse Matrix Computations},ed. J.R. Bunch and D.J.Rose, Academic Press, New York, 
1965, pp. 307-322.

\medskip
\noindent[\refstepcounter{sectrefs}\label {R:EBE-Golub}\thesectrefs]~Golub G.H., 
and Van Loan C.F.
Matrix Computations. \ti{The Johns Hopkins University Press}, Baltimore Maryland, 1983.

\end{document}

 


%
\documentclass[11pt]{report}
\usepackage{geometry} 
\geometry{letterpaper}

%---------------------------------------------
\setlength{\textheight}{630pt}
\setlength{\textwidth}{450pt}
\setlength{\oddsidemargin}{14pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex} 


%----------------------------------------
\usepackage{amsmath}
\usepackage{layout}
\usepackage{color}
\usepackage{hyphenat}
\usepackage{array}
\usepackage[us,12hr]{datetime}

%----------------------------------------------
\usepackage{fancyhdr} \pagestyle{fancy}
\setlength\headheight{15pt}
\lhead{\small{User's Guide - \textit{WARP3D}}}
\rhead{\small{\textit{Solution Parameters}}}
%\fancyfoot[L] {\textit{\small{Chapter {\thechapter}}\ \   (Updated: 5-14-2015)}}
\fancyfoot[L] {\small{\  Updated:  \today\ at \currenttime}}
\fancyfoot[C] {\small{\thesection-\thepage}}
\fancyfoot[R] {{\small{\textit{Model Definition}}}}

%---------------------------------------------------
\usepackage{graphicx}
\usepackage[labelformat=empty]{caption}
\numberwithin{equation}{section}

%---------------------------------------------
%     --- make section headers in helvetica ---
%
\frenchspacing
\usepackage{sectsty} 
\usepackage{xspace}
\allsectionsfont{\sffamily} 
\sectionfont{\large}
\usepackage[small,compact]{titlesec} % reduce white space around sections
%
%----------------------------------------------

%---------  local commands ---------------------
\newcommand{\ttt} {\texttt}  %typewriter text
\newcommand{\tb} {\textbf}
\newcommand{\nf} {\normalfont}
\newcommand{\df} {\dotfill}
\newcommand{\nin} {\noindent}
\newcommand{\bmf } {\boldsymbol }  %bold math symbol
\newcommand{\bsf } [1]{\textrm{\textit{#1}}\xspace}
\newcommand{\ul} {\underline}
\newcommand{\hv} {\mathsf}   %helvetica text inside an equation
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\ti}{\emph}
\newcommand{\bardelta}{\bar \delta}
\newcommand{\barDelta}{\bar \Delta}
\newcommand{\veps}{\varepsilon}

\newenvironment{offsetpar}[1]%
{\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
}
{\end{list}}

%
%
%        optional definition for bullet lists which
%        reduces white space.
%
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }
%

%-------------------------------------
\newcounter{sectrefs}
\setcounter{sectrefs}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}

%--------------------------------------
%--------------------------------------
%---------------------------------------

\begin{document}

\section{Solution Parameters}
%\layout

\noindent
The nonlinear (and dynamic) solution in WARP3D 
follows an incremental-iterative strategy with standard Newton iterations to eliminate
residual nodal forces caused by all sources of nonlinear behavior. The user has 
control over the solution procedures through a wide range of parameters. 
Each of the parameters has a built-in default value that may be redefined by 
the user. The user declares the values of these parameters before computation begins 
for the first load step; those values remain in effect unless modified by the user as 
the solution progresses through the load steps. New values for these parameters may 
be defined whenever the input translators accept new input lines. The most current 
values of the parameters then control subsequent computations over load steps.

The specification of solution parameters begins with a command of the form
\begin{align*}
& 
\begin{Bmatrix}
\hv{\ul{solution}} \\ \hv{\ul{nonlinear}}\\ \hv{\ul{dynamic}}
\end{Bmatrix}
\hv{(\ul{analysis})\ \ul{parameters}} 
\end{align*}
\nin and terminates whenever a command is given that does not define a parameter 
controlling the analysis. The following sections describe each of the parameters 
assignable by the user and the command syntax. An example defining values 
for selected solution parameters is shown below for reference.
\small
\begin{verbatim}
  solution parameters
   solution technique sparse direct  $ Pardiso solver in Intel MKL 
   time step 0.005
   maximum iterations 10             $ global Newton iteration per load step
   convergence test norm res tol 0.5
   extrapolate on                    $ displacements at start of new step
   adaptive solution on              $ for global Newton iterations
   divergence check on               $ early detection of non-converging solutions
   line search on details            $ each Newton iteration
   batch status messages on
   wall time limit on 7200  $ stop w/ restart if limit will be exceeded by next step
   material messages off
   trace solution on
   bbar stabilization on             $ For 8-node hexs
   consistent q-matrix off
       .
       .
   compute displacements for loading impact step 5000
\end{verbatim}
\normalsize

\subsection{Threads for WARP3D Execution}
\nin
WARP3D uses threads to achieve parallel execution throughout, 
primarily to process blocks of elements as described in Section 2.6, and in
the equation solvers.
The user sets the number of threads for parallel execution 
through  a 
system environment variable before initiating WARP3D execution. To set the 
number of threads in
 a Bash shell, for example, use a  command of the form
 
\small
\begin{verbatim}
   export OMP_NUM_THREADS=16
\end{verbatim}
\normalsize
\nin where here sets the number of threads to 16 for WARP3D. The default number 
of threads is 1. 

\subsection{Linear Equation Solvers}
\nin
Section 1.7 provides an overview of the various linear equation solvers 
and the best model characteristics and computer hardware for the
application of each solver. All solvers execute in parallel to utilize the hardware
resources efficiently. 

\subsubsection{Pardiso Sparse Direct Solver (Threads-Only Execution)}
\nin
The Pardiso sparse direct solver for symmetric and asymmetric systems of equations 
as implemented in the Intel Math Kernel Library (MKL) is the most widely applicable
solver. At present, asymmetric systems (structural symmetry with non-symmetric
numerical values) 
are generated for the tangent stiffness matrices of the crystal plasticity model. 
\footnote{Analyses using the crystal plasticity material model may also invoke the 
symmetric solvers -- a symmetrized version of the tangent stiffness is
created which will (generally) slow convergence of the global Newton iterations}
Pardiso is available on all three platforms (Windows, Linux, Mac OSX) in the 
threads-only execution mode of WARP3D.

The command to request the symmetric version of the solver is
\begin{align*}
&\hv{\ul{solution}\ (\ul{technique})\ \ul{sparse}\ \ul{direct}} 
\end{align*}
and for the asymmetric version
\begin{align*}
&\hv{\ul{solution}\ (\ul{technique})\ \ul{asymm}etric \ \ul{direct}} 
\end{align*}
where the optional word \ttt{sparse} may be included for readability.
This solver runs parallel using threads and shared memory.  An out-of-core
option (OOC) is available for very large models analyzed
on machines with reduced real memory.

This solver is recommended for analyses executing on computers that
provide shared memory (laptops, desktop and deskside machines).  In solving
a given model (number of nodes), the efficiency of Pardiso will
decrease at some point as the number of threads are increased. This occurs
because the number of equations becomes small relative to the number of
threads available (not enough parallel work to do)  or Pardiso 
running with that number of threads 
overwhelms the memory subsystem of the hardware to supply data to the
processors. Each new release of the Pardiso solver improves the
parallel efficiency, especially as new processor capabilities are introduced
(\eg longer vector lengths).

Nonlinear solutions with models having 500K-1M or more nodes are handled readily with the
Pardiso direct solver -- provided the computer has sufficient real memory to prevent
virtual memory paging to disk. 

The user sets the number of threads for parallel execution of Pardiso through  a 
system environment variable before initiating WARP3D execution. To set the 
number of threads for Pardiso in a Bash shell, for example,  use the  command 
\small
\begin{verbatim}
   export MKL_NUM_THREADS=#
\end{verbatim}
\normalsize
\nin  The default number 
of threads is 1. Most often the number of threads for MKL (Pardiso)
execution will be set  equal to the number of threads 
for WARP3D execution, \ie
\small
\begin{verbatim}
   export OMP_NUM_THREADS=16
   export MKL_NUM_THREADS=16
\end{verbatim}
\normalsize


The out-of-core (OOC) execution creates (potentially) large, binary disk files in a user specified 
directory to segment the factored equations such that the maximum (physical) 
memory use does not exceed a user-specified limit. The following additional commands
request OOC execution of the Pardiso solver and provide the additional user data.
\begin{align*}
& \hv{\ul{solver}\ \ul{out\mbox{-}of\mbox{-}core} }
\begin{Bmatrix}
\hv{\ul{on}} \\ \hv{\ul{off}}
\end{Bmatrix}
\\
&\hv{ \ul{solver}\ \ul{memory}\ <allowed\ memory\ in\ MB:\ integer>} \\
&\hv{ \ul{solver}\ \ul{scratch}\ \ul{directory} <string>} 
\end{align*}
\nin The default value is ``off''. The solver memory and solver directory commands become
inactive unless the out-of-core solver option is set to ``on''. The memory value sets the limit that
Pardiso may allocate for use in solving the equations. The value is specified in 
MB (mega-bytes). Thus, 1.5GB would be specified as 1500 in the command. 
The allowed memory should always be as large as possible. The default value for 
memory is 500MB. If not specified, the default scratch directory is
 ./warp3d\_ooc\_solver. Pardiso then creates scratch files with names 
./warp3d\_ooc\_solver.jnl, etc. (the solver creates scratch files in the working 
directory with these names). A directory specified with the name 
d:/crack\_job/small\_model for example in Windows implies that the 
directory d:/crack\_job already exists and that scratch files will be created 
with names d:/crack\_job/small\_model.jnl, ... Note the use of 
forward slashes (/) in file names for the Windows systems. 
Scratch files remain on disk following execution.

A complete example with the OOC option for the solver is
solution parameters
\small
\begin{verbatim}
 nonlinear analysis parameters
   solution technique sparse direct
   solver out-of-core on
   solver memory 900
   solver scratch directory "d:/crack_job/small_model"
   maximum iterations 10
   convergence test norm res tol 0.5
   time step 0.0005
      .
      .
\end{verbatim}
\normalsize

\subsubsection{Pardiso Sparse Iterative Solver  (Threads-Only Execution)}
\nin
The Pardiso sparse iterative solver for symmetric and asymmetric systems of equations 
as implemented in the Intel Math Kernel Library (MKL) is the next most widely applicable
solver. Select the symmetric solver with this command
\begin{align*}
&\hv{\ul{solution}\ (\ul{technique})\  \ul{sparse}\ \ul{iterative}} 
\end{align*}
and this command for the asymmetric version (see notes above about use
with the crystal plasticity model)
\begin{align*}
&\hv{\ul{solution}\ (\ul{technique})\  \ul{asymmetric}\ \ul{iterative}} 
\end{align*}
This solver runs parallel using threads and shared memory and employs
a Krylov subspace (conjugate gradient) method with the fully factorized equations as the
preconditioner. The sparse direct solver described above performs
the factorization. The solver runs with a fixed preconditioner for load
steps and Newton iterations until the convergence rate of the CG
iterations degrades to an unacceptable level. The current set of equations
is then re-factored to update the preconditioner. The iterative solution process does
not reduce the memory demands since the preconditioner uses 
the Choleski factorization of the assembled equations via the Pardiso direct solver.

For simulations with comparatively smooth behavior over time/loading,
this solver is often twice as fast (or more) 
for larger models as the direct solver since CG iterations are much
less expensive that factorization. The solver always returns a solution via 
factorization if required. No out-of-core option is available. The solver is invoked
with a tight convergence tolerance on the CG iterations (no user option available as yet).

For non-smooth problems with rapidly changing material properties, contact, crack extension,
etc., the iterative solver may provide minimal-to-no reduced solver times.

Set the number of threads for execution as described in the previous section. 

Numerical experiments are recommended for the user's class of simulations to
assess possible advantages from the iterative solver.


\subsubsection{Hypre Iterative Solver  (MPI + Threads Execution)}
\nin
For the very largest models with execution using a hybrid approach
(MPI+threads), the \ti{hypre} iterative solver (PCG) from Lawrence Livermore
National Laboratory is incorporated into WARP3D. This solver provides a family
of highly scalable preconditioners and is
suitable for models with millions of nodal displacements. 

\nin Key points:
\small
\squishlist
\item The finite element model must be decomposed into domains and domains into
blocks of elements. The number of MPI ranks during execution
must equal the number of 
model domains.
\item Hypre executes using the same MPI ranks as WARP3D.
\item The stiffness assembly process is performed with a distributed approach
over the MPI ranks which dramatically reduces memory requirements on
MPI rank 0. At present, the use of  tied-contact and/or user-defined multi-point constraint 
features in WARP3D disables the distributed assembly. For this
situation, element stiffness
matrices are collected on MPI rank 0 for assembly and enforcement of the
linear constraints. The assembled equations reflecting the tied/multi-point
constraints are then distributed to MPI ranks for hypre solution.
\item WARP3D supports the BoomerAMG (algebraic multi-grid) and the
ParaSails preconditioners. 
\item Hypre conjugate gradient iterations may fail to achieve convergence. 
If the user has selected the \ti{adaptive step size control} option in 
WARP3D solution parameters, the load step size will be reduced and the
solution process continued (see subsection below on adaptive step size
control). Initial testing with hypre indicates this process works very
well for models with rapidly changing nonlinear response.
\squishend
\normalsize
Select the hypre solver with the command sequence
\begin{align*}
&\hv{\ul{solution}\ (\ul{technique})\ \ul{hypre}} 
\end{align*}
\nin WARP3D processing of element blocks and hype code execution on each
MPI rank can make use of thread parallel processing as well.  Chapter 7 describes the process
to begin WARP3D execution using MPI with threads on the Linux platform. 
The user must specify the number of MPI ranks and the number of threads per rank
to not oversubscribe computing resources available on the hardware.

\ti{\ul{Recommendation}}: Experience at this time suggests 
that the number of threads per MPI rank
should be 2-4 (at most). WARP3D processing of element blocks using threads shows
extraordinary efficiency, but hypre shows little improvement in
efficiency with more than a few threads per MPI rank. At present then, models should
be set up for predominantly domain-based, MPI execution.

The hypre system offers many user selectable options. WARP3D exposes some of the key
options to enable changing of values by the user if necessary. Appropriate default 
values are defined by the
WARP3D input translators such that the minimum input shown in the above example
often suffices.  

WARP3D supports access to two preconditioners included in hypre: ParaSails and BoomerAMG.
ParaSails computes an approximate inverse preconditioner for the iterative solution of large, 
sparse systems of linear equations. It uses least-squares (Frobenius norm) minimization 
to compute a sparse approximate inverse. The sparsity pattern used is 
the pattern of a power of a sparsified matrix. ParaSails also uses a 
post-filtering technique to reduce the cost of applying the preconditioner. 

BoomerAMG is a parallel algebraic multigrid solver.  BoomerAMG 
generally produces a much better preconditioner than Parasails which improves
convergence of the PCG method.  In addition, for large models BoomerAMG exhibits more
parallel scalability than Parasails.  
Users are encouraged to experiment with the two preconditioners for their models.

Input commands for the hypre solver fall into three categories: (1) commands 
common to all preconditioners, (2) commands specific to Parasails, and (3) commands
specific to BoomerAMG.  The three tables below describe each of the input command sets in turn.

All options are prefaced by the keyword \ti{hypre}.

%
\begin{table}[htb]
\setlength{\extrarowheight}{5.0pt}
\small
\begin{tabular}[htb] { | p{1in} | p{0.5in} | p{4.0in} |  }
\hline 
Option & Default & Description \\
\hline \hline 
solver & pcg & Iterative method used to solve the system of linear equations.  Only pcg is implemented in WARP3D \\
tolerance & 1.0E-8  & L2-norm relative convergence tolerance on PCG solution residual \\
iterations & 10000 & maximum number of conjugate gradient iterations \\
preconditioner & parasails & alternatively try \ti{boomeramg} for better CG convergence  \\
printlevel & 0 & integer 0 to 3 indicating amount of information to output during solve (0 prints none, 3 prints most) \\
symmetry & 1 & parameter indicating symmetry of the ``A" matrix (do not change)  \\
balance & 0 & load balancing parameter \\ \hline
\end{tabular}
\caption{Table: General user-definable options for the hypre (PCG) solver}	
\normalsize
\end{table}


When running hypre with a large number of MPI ranks 
(greater than 128), we recommend setting the \ti{balance} parameter to 0.9, which turns on load 
balancing within hypre.  

Users will likely find that the default tolerance for convergence of the 
PCG iterations is too strict.  Numerical testing reveals it may often
be reduced to 1.0E-6 without loss of accuracy in the solution. Experimenting with the
tolerance for specific models may lead to computational savings.

%
\begin{table}[htb]
\setlength{\extrarowheight}{5.0pt}
\small
\begin{tabular}[htb] { | p{1in} | p{0.5in} | p{4.0in} |  }
\hline 
Option & Default & Description \\
\hline \hline 
levels & 1 & highest power of the matrix $\mathbf{A}$ to be used in constructing the preconditioner  \\
filter & 0.1 & sparsification factor applied to the approximate-inverse preconditioner \\
threshold & 0.1 & sparsification factor applied to the system of equations \\ \hline
\end{tabular}
\caption{Table: User-definable parameters for the Parasails preconditioner}	
\normalsize
\end{table}

With the Parasails preconditioner, slightly increasing the ``threshold" and ``filter" parameters 
(to something in the range of 0.1 to 0.15) can significantly improve performance on some problems.  Increasing the
``levels" parameter will improve the quality of the preconditioner, improving the convergence of the PCG method.  
However, increasing this parameter vastly increases the computational effort required to form the preconditioner.  
In most cases, users will be better off switching to BoomerAMG if convergence with Parasails is difficult.

%
\begin{table}[htb]
\setlength{\extrarowheight}{5.0pt}
\small
\begin{tabular}[htb] { | p{1in} | p{0.5in} | p{4.0in} |  }
\hline 
Option & Default & Description \\
\hline \hline 
coarsening & HMIS & type of coarsening to use in constructing coarse grids: CLJP, falgout, PMIS, or HMIS  \\
interpolation & multipass & type of interpolation used in transitioning between coarse and fine grids: classical, direct, standard, multipass or extended classical (ext\_classical) \\
relaxation & gs & relaxation technique used to smooth grid operators: jacobi or Gauss-Seidel (gs)  \\
max\_levels & 10 & maximum number of grids to construct (including original linear operator)  \\
mg\_threshold & 0.8 & strong connection threshold  \\
cycle\_type & V & type of MG cycle to use: V or W  \\
sweeps & 1 & number of relaxation passes before and after a grid level -- default cycle and sweep option results in a V(1,1) cycle  \\
agg\_levels & 1 & number of (top) levels where aggressive coarsening is applied \\
truncation & 0.0 & truncation factor to trim over-large interpolation/restriction stencils \\
wt\_relax & 1.0 & relaxation weight for smoother \\
wt\_outer & 1.0 & outer relaxation weight, used for applicable smoothers only \\
cs & F & whether to use the CS splitting (T/F) \\ \hline
\end{tabular}
\caption{Table: User-definable parameters for the BoomerAMG preconditioner}	
\normalsize
\end{table}

In BoomerAMG, parameters for the the smoother, the cycle type, and the number of levels will, in general, only effect the
quality of the preconditioner, not its parallel efficiency.  The remainder of the parameters effect both convergence
and parallel performance.  For additional details on these parameters, BoomerAMG, and algebraic multigrid methods 
in general, consult the hypre documentation and related papers and reports.



\subsection{Dynamic Analysis Parameters}
\nin
The time increment over each load step and the $\beta$ factor 
for Newmark time integration scheme are defined by the commands
\begin{align*}
&\hv{\ul{time}\ \ul{step}\ <number>} \\
&\hv{\ul{newmark}\ \ul{beta}\ <number>} 
\end{align*}
The default time step size is $10^6$ and the default value of the Newmark $\beta$ 
factor is $1/4$. Notes:
\squishlist
\item
The time step must be positive
\item
Strain-rate dependent solutions without inertia effects (\eg viscoplasticity) may 
be obtained by setting a realistic time step size for a model with zero mass denisty
\item
The time step and $\beta$ may be adjusted between load increments as needed
\item For specified values of $\beta \ge 1.0$, the input processors set $\gamma=1.5$ in 
Newmark's method. This action increases algorithmic damping to maximum levels.
\item
All solutions are actually computed with the fully implicit dynamic 
formulation. Static analyses are simply solutions with a very large time step.
\squishend

\subsection{Global Newton Iteration Parameters}

\subsubsection{Maximum Iteration Limit}
\nin The upper limit on the number of (global) Newton iterations per load
step is defined by
\begin{align*}
&\hv{\ul{max}imum\  \ul{iter}ations <integer> }
\end{align*}
The default limit is 10.

\subsubsection{Minimum Iteration Limit}
\nin The minimum number of Newton iterations defaults to 2. 
 For linear analyses this should be set to 1.
 \begin{align*}
&\hv{\ul{min}imum\  \ul{iter}ations <integer> }
\end{align*}

\subsubsection{Nonconvergent Solutions}
\nin By default, the solution terminates when the solution appears to be
diverging and the user has not requested adaptive subincrements.
The action to take for nonconverging load (time) steps
is set with the command

\begin{align*}
&\hv{\ul{nonconverge}nt\  \ul{sol}utions }
\begin{Bmatrix}
\hv{\ul{stop}}\\ \hv{\ul{continue}}
\end{Bmatrix}
\end{align*}

\noindent The default is to \ti{stop} the simulation.

\subsubsection{Diverging Iterations Check}
\nin In solutions with difficult convergence behavior, the Newton iterations for a
step may begin to reveal increasing values of the norm of the nodal 
residual forces vector
(the ``norm").
After completion of Newton iteration 3, 
the nonlinear solution processors  check
the behavior of this norm. If this norm increases for two 
consecutive iterations (including iterations 2 and 3), the iterations are
considered to be diverging --  the adaptive process is
immediately triggered. 

The \ti{strict} option enforces a more stringent definition
for a diverging solution. If the norm of the residual forces increases
at any iteration, the iterations are
considered to be diverging --  the adaptive process is immediately triggered. 

If the user has turned off the adaptive process, the analysis
terminates.

The checking of iterations for divergence is controlled with the command
\begin{align*}
&\hv{\ul{diver}gence\  (\ul{ch}eck })
\begin{Bmatrix}
\hv{\ul{on}}\\ \hv{\ul{off}}
\end{Bmatrix}
\hv{(\ul{strict})}
\end{align*}

\noindent The default value is ``on" without ``strict".

\subsubsection{Convergence Tests}
\nin Six types of tests are available to assess convergence of the global Newton 
iterations. The table defines the solution quantities appearing in the convergence
tests.
%
\begin{table}[h]	\small
{
\setlength{\extrarowheight}{5.0pt}
\begin{tabular}[htb]{p{1in} p{5in}  }
$||\bmf{R}_k||$ &Euclidean norm of the residual force vector 
for the model following solution of iteration $k$ of the step\\ 
$max\left[abs\ \bmf{R}_k^{(i)} \right]$ & maximum (absolute) entry in the residual
force vector for the model following solution for iteration $k$ of the 
step (only active dof are considered) \\
$||\bmf{P}||$ & Euclidean norm of the total force vector applied to the model 
(includes reactions at dof with absolute constraints, contact forces and inertia forces).
Does not include former reaction
forces at nodes undergoing release or nodes of elements
being killed\\
$||\Delta\bmf{u}_1||$ & Euclidean norm of the incremental displacement vector for 
the model computed during iteration 1 of the load step \\
$||\Delta\bmf{u}_k||$ & Euclidean norm of the incremental displacement vector 
for the model computed during iteration $k$ of the load step\\
$max\left[abs\ \bmf{u}_k^{(i)} \right]$ & maximum (absolute) entry in the displacement 
vector for the model following solution for iteration $k$ of the step\\
$\overline q$ & the numerical average of all forces (absolute value) applied to the nodes
 including: separate, internal element forces due to stresses, inertia forces, contact forces,
reaction forces and externally applied nodal/element forces
\end{tabular}
}
\caption{Table: Quantities for Definition of Convergence Tests on Global Newton Iterations}
 \normalsize	
\end{table}

\nin When multi-point constraints (MPCs) are active in the solution, terms of the 
residual force vector $\bmf{R}$ at which Lagrange forces act to enforce the 
constraints are not included in convergence tests. These 
always define a set of self-equilibrating forces.

The computation of $\overline q$ is as follows
\begin{equation*}
\overline q = \frac{1}{\overline N} \left [ \sum\limits_k^{N_e}\sum\limits_i^{3n_e} |I_{(i)}|^{(k)}
+\sum\limits_j^{3N_n}    \left ( |f_{(j)}^c| +|f_{(j)}^I| + |f_{(j)}^P|
\right )\right ]
\end{equation*}
\nin where $N_e, n_e, N_n$ are the number of elements, the number of nodes on an element and the number 
of model nodes. Also, $\bmf{I}, \bmf{f}^c, \bmf{f}^I, \bmf{f}^P$ are structure level, nodal vectors
of contact forces, inertial and applied forces (nodal loads, element equivalent nodal loads and
computed reaction forces). $\overline N$ is the total number of entries included in $\overline q$.
This quantity is especially useful in models with residual stresses and thermal stresses, for example,
where the applied nodal/element forces and reactions may vanish at some time during the
solution -- but $\overline q$ rarely vanishes.

Using these quantities, the six convergence tests are defined in the table.
Here $|| \cdot ||$ denotes the Euclidean norm. Multiple convergence tests may 
be defined; convergence of the solution and termination of additional
iterations requires satisfaction of all tests. For tests 1-4, tolerance values are 
specified in (\%); thus, a user tolerance of 0.01 (\%) is reasonably strict and 
often used. For tests 5-6, the tolerance values refer to absolute values of the displacements.
Tolerance values of $10^{-6}$ have been used successfully in various models. Newton iterations
with good convergence properties should decrease the corrective nodal displacements by two
orders of magnitude in each iteration (after the first few iterations of the step).
The user specified tolerance can exert a dramatic impact on the required number of 
Newton iterations and the total CPU time. Excessively tight tolerances seldom provide
 real improvements in solutions. 

%
\begin{table}[htb] \small 
\centering
{
\setlength{\extrarowheight}{5.0pt}
\begin{tabular}{ p{0.5in} l  }
Test 1: & $||\Delta\bmf{u}_k|| \le$ (user tol/100 )$ \times ||\Delta\bmf{u}_1||$ \\ 
Test 2: & $||\bmf{R}_k|| \le$ (user tol/100 )$ \times ||\bmf{P}||$ \\
Test 3: & $max\left[abs\ \bmf{u}_k^{(i)} \right] \le$  (user tol/100 )$ \times ||\Delta\bmf{u}_1||$ \\
Test 4: & $max\left[abs\ \bmf{R}_k^{(i)} \right]\le$  (user tol/100 )$ \times\, \bar q$\\
Test 5: & $||\Delta\bmf{u}_k|| \le$ user tol \\ 
Test 6: & $max\left[abs\ \bmf{u}_k^{(i)} \right] \le$  user tol \\
\end{tabular}
} 
\caption{Table: Convergence Tests for Global Newton Iterations}
\normalsize	
\end{table}

Tests 1-4 employ  \ti{relative} tolerance tests and the choice of physical units 
affects the corresponding absolute tolerance. For example, a user tol of 0.01 
that may be suitable for a problem with forces in units of kips might be absurdly 
stringent if the force units in the same problem are given in pounds-force instead. 
This has importance, for example, in fracture problems where the actual 
residual force values on nodes in the crack front region must be controlled carefully.

The  commands to define the convergence tests has the form
\begin{align*}
&\hv{\ul{conver}gence\ (\ul{test}s)\ \left[ <test\ type> \right]}
\end{align*}

\nin where multiple tests may be defined in a single command and 
continued over multiple input lines with commas. Each appearance of the
\ti{convergence} command deletes previously defined tests.

Tests 1 and 2 are defined with [$<$test type$>$]
\begin{align*}
& \hv{ \ul{norm} }
\begin{Bmatrix} 
\hv{ \ul{displac}ement} \\ \hv{\ul{res}idual }
\end{Bmatrix}
\hv{ \ul{tol}erance <tolerance:number> }\\ 
\end{align*}
Tests 3 and 4 are defined with [$<$test type$>$]
\begin{align*}
& \hv{ \ul{max}imum }
\begin{Bmatrix} 
\hv{ \ul{displac}ement} \\ \hv{\ul{res}idual }
\end{Bmatrix}
\hv{ \ul{tol}erance <tolerance:number> }\\ 
\end{align*}
Tests 5 and 6 are defined with [$<$test type$>$]
\begin{align*}
\hv{ \ul{norm}\ \ul{displac}ement\ \ul{absol}ute\  \ul{tol}erance <tolerance:number> }
\end{align*}
\begin{align*}
\hv{ \ul{max}imum\ \ul{displac}ement\ \ul{absol}ute\  \ul{tol}erance <tolerance:number> }
\end{align*}

\nin An example of 
convergence test commands is:
\small
\begin{verbatim}
  nonlinear analysis parameters
    maximum iterations 10
    convergence tests norm res tol 0.1 maximum displ tol 0.01,
       maximum displacement absolute tol 1.0e-06
    nonconvergent solutions continue
\end{verbatim}
\normalsize


\subsection{Line Search}
\nin Line search augments the computations for a single Newton iteration to potentially
improve the convergence rate -- especially for situations when the standard
Newton iteration predicts too large a change in the corrective displacement
increment. See Section 1.6 for more details.

Each cycle of the line search for a global Newton iteration reduces the
magnitude of the current (iterative) corrective displacement increment
through a simple backtracking scheme, updates
strains, stresses and forms a new residual nodal force vector.  A residual force
vector orthogonal to the corrective displacement is the ideal outcome (\ie
suggests a local minimum potential energy condition). The dot product of the
residual and corrective displacement vectors  (say B) is compared to the
dot product of the residual vector and displacement increment to start the 
iteration (say A).  If a sufficient reduction of B/A has occurred, the line search cycles stop and the
scaled (corrective) displacement vector and corresponding residual vector (in the dot product)
are used to start the next global Newton iteration.

The line search command has the form 
\begin{align*}
&\hv{\ul{line}\  \ul{search}}
\begin{Bmatrix}
\hv{\ul{on}}\\ \hv{\ul{off}}
\end{Bmatrix}
\hv{<options>}
\end{align*}

\nin where the $\hv{<options>}$ are listed here in a short table.


%
\begin{table}[htb] \small 
\centering
{
\setlength{\extrarowheight}{5.0pt}
\begin{tabular}{ p{0.8in} l c  }
Keyword & Quantity & Default Value \\
$\hv{rho}$ & backtrack reduction factor, $\rho$ & 0.7\\ 
$\hv{min\_step}$ & minimum step length, $\alpha_{min}$& 0.01 \\
$\hv{slack\_toler}$ & slack tolerance test, $r_{tol}$ &0.5 \\
$\hv{details}$ & show result of each line search cycle& no details \\
\end{tabular}
} 
\caption{Table: Line Search Options}
\normalsize	
\end{table}

\nin When line search is \ti{on} but the solution with standard
Newton satisfies the slack tolerance condition, the computational cost
(\ie for the line search check) 
is one additional pass to compute strain increments, update stresses and
a new model level residual force vector.

\nin \ul{Line search is \ti{off} by default}. Nevertheless, turning on linear 
search for the simulation of new classes of models is strongly recommended.
Once suitable load step sizes are known for similar analyses
during parameter studies, for example, line search may be turned off.


\subsection{Adaptive Step Size Control}
\nin In a nonlinear analysis (static or dynamic), it is often difficult to estimate a priori the 
appropriate load step sizes that provide acceptable convergence of the 
global Newton iterations. 
WARP3D provides a simple facility to reduce automatically load step (and time step) 
sizes when the solution appears to be diverging or converging slowly.  By default, the 
adaptive step size feature is \ti{not} used.

The adaptive algorithm is very simple. When the solution is found to be non-converging,
the load step (and time step) is subdivided 
into four (4) equal increments and the solution for the load step is re-attempted. Steps are not 
renumbered during this process so that output messages indicate four solutions of the 
same load step with a fractional part, \eg a step 32.25.

A load step is considered non-converging when any of the following conditions
is met

\squishlist
\item The user-specified maximum number of Newton iterations is reached
\item The \ti{divergence} check determines the solution is not converging
\item A material model requests an immediate adaptive reduction - likely because a
material level iterative procedure failed to converge 
\item In geometrically nonlinear analyses, unusually large displacement increments may lead to 
a zero or negative deformation Jacobian at Gauss points in elements
\squishend

If  the adaptive option is ``off" when one of the above conditions is reached,
 the solution processor terminates execution.

If the solution does not converge in any one of the 4 subincrements, that subincrement is 
further subdivided into four more increments and the solution re-attempted. Only two such levels
of step reduction are permitted; nonconverged solutions at the second level cause program 
termination. In many cases, the first level of step reduction is sufficient. In other cases, one or 
more of the 0.25 fractions must be subdivided to obtain convergence. The adaptive algorithm 
performs level two reduction only for the level one fractions that do not converge.

The command to control adaptive load step sizes is
\begin{align*}
&\hv{\ul{adaptive}\  (\ul{sol}ution) }
\begin{Bmatrix}
\hv{\ul{on}}\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}

When the adaptive procedure re-attempts the analysis for a load step or subincrement, it forces 
\ti{extrapolation off} for the first Newton iteration which causes the linear
elastic $\mathbf{D}_{el}$ (at that temperature) to be used. This is required 
since the current estimate for the solution at $n+1$ is not valid for use to recompute element 
matrices. The full Newton process resumes at the next iteration. WARP3D manager routines 
handle these processes automatically.

Adaptive load step control is strongly recommended for users attempting the nonlinear solution 
of new classes of problems until experience with the convergence characteristics are known. 
For parametric studies of problems with well-known convergence characteristics, adaptive 
load step control should be avaoided as it often dramatically increases analysis run times 
(the code repeatedly learns what size steps converge!). Analyses run much faster when 
the user specifies load step sizes known to exhibit good convergence characteristics.

\nin \ti{\ul{Non-zero constraints}}: When a load step is subdivided, the non-zero constraints 
(\eg $\Delta u_{10}=0.1$) imposed by the user are reduced by the same adaptive factors as 
the step load. The actual constraint values specified by the user and stored in program 
data structure are not modified. Rather, scaled values are imposed during the equation 
solving process.

\subsection{Displacement Extrapolation}
\nin The use of an extrapolated, nodal  displacement vector frequently reduces the 
number of global Newton iterations -- especially for relatively
smooth model responses. All terms of the incremental displacements 
computed for the solution over
step $n-1\rightarrow n$ are scaled by the same factor and applied to the model to start 
the iterative Newton solution from $n\rightarrow n+1$. 

The extrapolated displacement vector is employed at the beginning of load step $n+1$ to compute 
a set of incremental nodal forces for application to the model  during the step
together with any applied incremental loads (see more extensive discussion in
Section 1.6).  Strain increments 
$(\Delta \bmf{\veps})$ are computed from the extrapolated displacements and adjusted
for temperature effects;  stress increments follow from usual material model
stress updates and include thermal and rate effects. History dependent (state) material data are discarded
since the iteration 0 computations with extrapolation aim to
 compute a better approximation of the incremental loading terms that depend
 on material response (thermal properties, creep strain, other initial strains) and to
 obtain  corresponding consistent $\mathbf{D}$ matrices to form the 
 model stiffness to use in (real) Newton iteration 1.
 
 When extrapolation is \ti{off}, the material models provide the linear-elastic 
 $\mathbf{D}_{el}$ (for the temperature at $n+1$) and updated stresses for thermal and initial
 strain increments. WARP3D then adjusts the
 iteration 0  stresses to include the contribution from strain increments caused by
 nonzero, imposed nodal displacements using the  $\mathbf{D}_{el}$.
 

The extrapolation procedure\footnote{The extrapolation procedure 
in WARP3D follows partially the scheme used in Abaqus.
By default, displacement extrapolation is \ti{on} on for Abaqus \ti{increments} except increment 1 of
a *STEP.  Abaqus anticipates that loading may be non-proportional across *STEPs but
increments within a *STEP will tend have a comparatively smooth response and thus
benefit from extrapolation.}
 requires (and verifies) the existence of proportional,
incremental loading from steps $n \rightarrow n+1$. Consider the following
example. The definition of loading patterns (say A and B) appearing step $10$ has user-specified
multipliers of 1.5 for A and 2.0 for B, and a 3.0 multiplier specified for the
\ti{constraints} (assuming there are non-zero values specified in the constraints).
For step $9$, the multipliers are specified to be 0.75 for A,
1.0 for B and 1.5 for the \ti{constraints}.  The input would be

\small
\begin{verbatim}
  loading test
     nonlinear
       .
       .
       step 9   A  0.75  B 1.0   constraints 1.5
       step 10  A 1.5   B 2.0   constraints 3.0
       .
       .
\end{verbatim}
\normalsize
In this case, the incremental loading patterns
and constraints for step 10 are \ti{proportional} to those of step 9, having a
single scale factor of 2.0 for each pattern and the imposed
displacements. The incremental displacements computed during step $9\rightarrow 10$
are multiplied by 2.0 to start the solution of step $10 \rightarrow 11$. 

When the solution procedures detect a condition of non-proportional, incremental
loading, the extrapolation of displacements is suspended for the load step and
automatically resumed for the next step if proportionality is again
detected. This procedure proves necessary, for example, at steps in
cyclic loading histories at which the loading is reversed. The use of 
displacement extrapolation on steps
with load reversals often leads to non-convergent Newton iterations. 
This forces use of the linear-elastic $\mathbf{D}_{el}$ to begin the next step
(at the corresponding temperature).

Simple cases with proportionality that arise most often 
include: (1) the same, single loading pattern 
across load steps and user-specified constraint values are all zero; (2) loading of  the
model only through non-zero constraints. Further, if all specified constraints
have zero values, the multiplier on constraints is immaterial and is neglected in the
verification of proportionality. 



The command to control displacement extrapolation is
\begin{align*}
&\hv{\ul{extrapol}ate (\ul{sol}ution) }
\begin{Bmatrix}
\hv{\ul{on}}\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}

\nin The default value for extrapolation is \ti{on}.

 
\subsection{Batch Status Messages}
\nin During solution of a large nonlinear problem in batch mode, it often proves convenient to 
have occasional information about the progress of the solution (load step/iteration number, 
convergence rate, etc.) WARP3D provides an option to produce status messages 
independent of the normal (standard) output file for the job. A status file is 
updated after each equilibrium if each step. Unless the file name is specified by the user
(see below),  the file is named 
$<$structure id$>$.batch\_messages. Linux/OS X users can invoke the tail 
command on this file during execution to examine the 
last few lines. The typical last few lines of this file can appear as:

\footnotesize
\begin{verbatim}
  convergence tests step:      7 iter:  2 wall time:     10.5  step fraction:  1.0000
    maximum residual force:   0.693863E-02 @ node:      2
    test 2: norm resid, total loads: 0.15690E-01 0.36162E+02 ratio*100:  0.04339  (passed)
    test 4: max residual, avg force: 0.69386E-02 0.15089E+01 ratio*100:  0.45986  (passed)
\end{verbatim} 
\normalsize

\nin If the batch message file exists from a previous analysis, the new information 
overwrites the old file. By default, no batch message files are written. 
The command to control batch messages is
\begin{align*}
&\hv{\ul{batch}\  (\ul{mess}ages) }
\begin{Bmatrix}
\hv{\ul{on}}\\ \hv{\ul{off}}
\end{Bmatrix}
\hv{(file\ <name:label\ or\ string>)}
\end{align*}

\subsection{Wall Clock Time Limit}
\nin On some systems, batch jobs are executed with a user specified limit set on the wall time for 
the job. If execution exceeds the wall time limit, the program is aborted by 
the operating system and all results after the last written restart file are lost. Estimating the 
required wall time for highly nonlinear problems may be very difficult, especially when 
similar problems have not been executed previously.

To help users with this problem, WARP3D provides its own wall time limit feature. The user 
sets the allowable wall time (in secs) for the job. At the beginning of the solution 
for load step $n+1$, WARP3D assumes that the solution time for the step is the same as 
the time required the solution of load step $n$. The total wall time estimated to advance 
the solution through load step $n+1$ is computed using this procedure and compared 
to the user specified limit. If the estimated time exceeds 90\% of the user limit, 
WARP3D writes a restart file named \tt{xxxxx\_overtime\_db} \nf  load step $n$ 
and terminates the 
job (\tt{xxxxx} \nf denotes the structure name).

The command to control this option is:
\begin{align*}
&\hv{\ul{wall}\  (\ul{time}) (\ul{limit}) }
\begin{Bmatrix}
\hv{\ul{on} <limit:secs>}\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}
\nin By default the wall time limit feature is ``off''.


\subsection{Material Model Messages}
\nin The material models have built-in features to print status messages during stress update. 
An option is provided to suppress all such informative messages generated by material models. 
Messages about severe conditions in the material models are not suppressed with this 
option. For example, the material model may request an immediate load step reduction 
when adaptive load control is enabled. In such cases, the material model prints a message 
to this effect with the reason it requests a load step reduction. 
The command to control printing of informative material messages is:
\begin{align*}
&\hv{\ul{mater}ial\  (\ul{mess}ages)  }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}
\nin Material messages are ``on'' by default.

\subsection{Solution Status Messages}
\nin The nonlinear solution process for a step and iteration involves many processes such as 
stiffness update, strain update, stress update, convergence tests etc. WARP3D outputs 
messages indicating when these processes start-finish and the detailed results 
for convergence tests. For users familiar with the code, most all of these 
messages can be suppressed with the \ti{trace} command that has the form:
\begin{align*}
&\hv{\ul{trace}\  \ul{sol}ution  }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}
\nin Tracing is ``on'' by default. This option has no effect on 
the batch message feature.

\subsection{Residual Loads Printing}
\nin Residual forces at nodes may be printed during Newton iterations to facilitate debugging 
of problems that exhibit unusual convergence. To request printing of residual loads, 
use the command
\begin{align*}
&\hv{\ul{print}\  \ul{residual}\  (\ul{loads})\  (\ul{for}) \  (\ul{iter}ations)\  <integer\ list>  }
\end{align*}
\nin Residual loads printing is ``off'' by default.

\subsection{B-Bar Element Stabilization}
\nin The $\bar B$ modification of the 8-node, trilinear element has the potential to introduce 
hourglass modes. Section 3.1.7 describes a simple procedure that can often suppress such modes. 
The user controls the amount of stabilization with the command
\begin{align*}
&\hv{\ul{bbar}\  \ul{stabil}ization\  (\ul{factor})\  <number>  }
\end{align*}
\nin where the numerical value ranges from 0.0 (no stabilization) to 1.0 (no $\bar B$). 
The default value for this factor is 0.0. Values not exceeding 0.10 are often used.

\subsection{ Consistent [Q] Matrix}
\nin The consistent tangent moduli for the incremental plasticity models (mises, cyclic, gurson) 
include the so-called $[\bmf{Q}]$ matrix for the finite strain formulation (see Section 1.9.4). 
This matrix most often enhances the convergence rate of global Newton iterations 
but there are occasionally instances when it slows convergence. 
\nin An option exists to omit the $[\bmf{Q}]$ contribution under control of the user. 
The user controls this option with the command <
\begin{align*}
&\hv{\ul{consistent}\  \ul{q}(\ul{\mbox{-}matrix})  }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}
\nin The default value is ``on''.

\subsection{User Solution Parameters Routine}
\nin A user-written subroutine may be called before the solution begins for each
load (time) step in the analysis. This routine is passed: (1) values the key solution
parameters described in this section, (2) the current definition of incremental
loading for the next
load (time) step, and (3) the history of global convergence for the
past five load (time) steps. With this information, the user routine may modify values
of the solution parameters (\eg time step, maximum iterations, convergence tests
and tolerances, bbar stabilization factor, etc.). The definition of
incremental loading for the next step may also be modified based on convergence
histories of recent steps.

The user subroutine is executed only if explicitly requested thru the
command
\begin{align*}
&\hv{\ul{user\_routine}\    }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}
\nin The default value is ``off''. See the appendix on user subroutines
for additional details.

\subsection{Resetting the Global Load Reduction Factor}
\nin The crack growth processors may invoke a permanent load reduction on the analysis 
when certain user-specified criteria are met. For example, when the increase in porosity in the 
Gurson model over a load step exceeds a user-specified limit, a permanent 50\% reduction 
is imposed on subsequent load step sizes. The reduction often improves the quality of the 
computed response. If necessary, further reductions in the load step sizes are imposed
by the crack growth processors.

While these load reductions often improve the solution quality, the code does not automatically 
restore the original (user-defined) load step sizes even when the reductions are no longer 
needed, for example, if the user changes the load step sizes. To alleviate this issue, 
the user can reset the load reduction factor to any desired value between load steps as part of 
the solution parameters. The command has the form
\begin{align*}
&\hv{\ul{reset}\  \ul{load}\  \ul{reduction}\  (\ul{factor})\  <factor:number>  }
\end{align*}
\nin where the value of 1.0 causes the user-defined load step sizes to be used, a value of 0.5 reduces the 
user-defined step sizes by 50\%, etc. This load reduction factor is saved in the restart file 
and its use continued on a restart.

\subsection{Output of Sparse Format Equilibrium Equations}
\nin To support exploration of new equation solvers, we have added a new feature to WARP3D 
to output the assembled structure stiffness matrix using a sparse format. This command 
actually outputs the equilibrium equations in sparse format, ready for solution. The equations 
include the effects of contact forces and have multi-point constraints from tied 
contact already enforced. The command to invoke this option is:
\begin{align*}
&\hv{\ul{sparse}\  (\ul{stiffness})\  (\ul{output})  }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\begin{Bmatrix}
\hv{\ul{binary} }\\ \hv{\ul{formatted}}
\end{Bmatrix}
\hv{\ \ \ul{file}\ <file name: string>  }
\end{align*}
\nin Once the file is written, the command is cancelled to prevent re-writing during each 
equation solve. The user can turn on the option again at any time by re-entering the command. 

The sparse storage format adopted in WARP3D has the following structure (illustrated 
using a small set of equations):
\small
\begin{verbatim}
               sparse matrix storage format
               ----------------------------

                1    2    3    4     5     6

          1  | 100   1    2                5  |  | x1 |     | 201 |
          2  |     200    6    7           9  |  | x2 |     | 202 |
          3  |          300   10    11    12  |  | x3 |     | 203 |
      a = 4  |                400   13    14  |  | x4 |  =  | 204 |
          5  |                     500    15  |  | x5 |     | 205 |
          6  |                           600  |  | x6 |     | 206 |

     number of equations    = 6

     number of coefficients = 12


     k_ptrs    = { 3, 3, 3, 2, 1, 0}

     k_indexes = { 2, 3, 6,  3, 4, 6,  4, 5, 6,  5, 6,  6}

     k_coeffs  = { 1, 2, 5,  6, 7, 9, 10,11,12, 13,14, 15}

     k_diag    = { 100, 200, 300, 400, 500, 600}

     k_rhs     = { 201, 202, 203, 204, 205, 206}
\end{verbatim}
\normalsize

This file can be written to a unformatted \ti{stream} or formatted sequential file. 
The stream file type omits writing of the usual Fortran record length 
information on the file. This makes the file structure
match that  of the stream of bytes concept used in C and C++.

Fortran statements used to open and 
write the file are given below:
\small
\begin{verbatim}
  stream unformatted file
  -----------------------
        open(unit=fileno, file=sparse_stiff_file_name,
     &       status='replace', access='stream', form='unformatted',
     &       iostat=open_result  )

          write(fileno) neqns, ncoeff
          write(fileno) (k_diag(i), i=1,neqns)
          write(fileno) (p_vec(i), i=1,neqns)
          write(fileno) (k_coeffs(i), i=1,ncoeff)
          write(fileno) (k_ptrs(i), i=1,neqns)
          write(fileno) (k_indexes(i), i=1,ncoeff)

  formatted file
  --------------
        open(unit=fileno,file=sparse_stiff_file_name,
     &       status='replace', access='sequential', form='formatted',
     &       iostat=open_result  )

      write(fileno,9500) neqns, ncoeff
      write(fileno,9510) (k_diag(i), i=1,neqns)
      write(fileno,9510) (p_vec(i), i=1,neqns)
      write(fileno,9510) (k_coeffs(i), i=1,ncoeff)
      write(fileno,9520) (k_ptrs(i), i=1,neqns)
      write(fileno,9520) (k_indexes(i), i=1,ncoeff)
 9500 format(2i10)
 9510 format(4e20.12)
 9520 format(7i12)
\end{verbatim}
\normalsize

\subsection{Output of MPCs for Tied Contact}
\nin The tie mesh processors generate multi-point constraints to enforce the 
displacements across topologically dissimilar element faces. 
An output table of the generated constraints may be requested with the following command:
\begin{align*}
&\hv{\ul{display}\  (\ul{tied})\  (\ul{mesh})\  \ul{mpcs}  }
\begin{Bmatrix}
\hv{\ul{on} }\\ \hv{\ul{off}}
\end{Bmatrix}
\end{align*}

\end{document}

 

